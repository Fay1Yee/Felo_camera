#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
from datetime import datetime, timedelta
from collections import defaultdict

def create_final_import_files():
    """åˆ›å»ºæœ€ç»ˆçš„å¯¼å…¥æ–‡ä»¶ï¼Œç¡®ä¿ä¸åº”ç”¨ç¨‹åºå®Œå…¨å…¼å®¹"""
    
    # è¯»å–å¤„ç†å¥½çš„æ•°æ®
    with open('import_log.json', 'r', encoding='utf-8') as f:
        import_data = json.load(f)
    
    print("=== åˆ›å»ºæœ€ç»ˆå¯¼å…¥æ–‡ä»¶ ===")
    print(f"å¤„ç†è®°å½•æ•°: {len(import_data)}")
    
    # 1. åˆ›å»ºæ ‡å‡†çš„å® ç‰©æ´»åŠ¨æ•°æ®æ–‡ä»¶ï¼ˆç”¨äºåº”ç”¨ç¨‹åºæ–‡ä»¶å¯¼å…¥ï¼‰
    with open('final_pet_activity_data.txt', 'w', encoding='utf-8') as f:
        f.write("timestamp\tcategory\tconfidence\treasons\n")
        
        for record in import_data:
            timestamp = record['timestamp']
            category = record['category']
            confidence = record['confidence']
            reasons = json.dumps(record['reasons'], ensure_ascii=False)
            
            f.write(f"{timestamp}\t{category}\t{confidence}\t{reasons}\n")
    
    # 2. åˆ›å»ºç»Ÿè®¡æŠ¥è¡¨æ•°æ®
    stats_data = generate_statistics_data(import_data)
    with open('statistics_data.json', 'w', encoding='utf-8') as f:
        json.dump(stats_data, f, ensure_ascii=False, indent=2)
    
    # 3. åˆ›å»ºè¡Œä¸ºåˆ†ææ•°æ®
    behavior_data = generate_behavior_analysis(import_data)
    with open('behavior_analysis_data.json', 'w', encoding='utf-8') as f:
        json.dump(behavior_data, f, ensure_ascii=False, indent=2)
    
    # 4. åˆ›å»ºæ—¶é—´çº¿æ•°æ®
    timeline_data = generate_timeline_data(import_data)
    with open('timeline_data.json', 'w', encoding='utf-8') as f:
        json.dump(timeline_data, f, ensure_ascii=False, indent=2)
    
    print("âœ… æœ€ç»ˆå¯¼å…¥æ–‡ä»¶åˆ›å»ºå®Œæˆ")
    print("ğŸ“ æ–‡ä»¶åˆ—è¡¨:")
    print("  - final_pet_activity_data.txt (æ ‡å‡†æ ¼å¼ï¼Œç”¨äºåº”ç”¨ç¨‹åºå¯¼å…¥)")
    print("  - statistics_data.json (ç»Ÿè®¡æŠ¥è¡¨æ•°æ®)")
    print("  - behavior_analysis_data.json (è¡Œä¸ºåˆ†ææ•°æ®)")
    print("  - timeline_data.json (æ—¶é—´çº¿æ•°æ®)")
    
    return {
        'total_records': len(import_data),
        'files_created': 4,
        'statistics': stats_data,
        'behavior_analysis': behavior_data
    }

def generate_statistics_data(import_data):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥è¡¨æ•°æ®"""
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    daily_stats = defaultdict(lambda: {
        'total_activities': 0,
        'categories': defaultdict(int),
        'avg_confidence': 0,
        'confidence_sum': 0
    })
    
    # æŒ‰å°æ—¶åˆ†ç»„
    hourly_stats = defaultdict(int)
    
    # æ€»ä½“ç»Ÿè®¡
    category_totals = defaultdict(int)
    total_confidence = 0
    
    for record in import_data:
        dt = datetime.fromisoformat(record['timestamp'])
        date_key = dt.strftime('%Y-%m-%d')
        hour_key = dt.hour
        category = record['category']
        confidence = record['confidence']
        
        # æ—¥ç»Ÿè®¡
        daily_stats[date_key]['total_activities'] += 1
        daily_stats[date_key]['categories'][category] += 1
        daily_stats[date_key]['confidence_sum'] += confidence
        
        # å°æ—¶ç»Ÿè®¡
        hourly_stats[hour_key] += 1
        
        # æ€»ä½“ç»Ÿè®¡
        category_totals[category] += 1
        total_confidence += confidence
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
    for date_key in daily_stats:
        stats = daily_stats[date_key]
        stats['avg_confidence'] = stats['confidence_sum'] / stats['total_activities']
        del stats['confidence_sum']  # åˆ é™¤ä¸´æ—¶å­—æ®µ
    
    return {
        'summary': {
            'total_records': len(import_data),
            'total_categories': len(category_totals),
            'avg_confidence': total_confidence / len(import_data),
            'date_range': {
                'start': min(record['timestamp'] for record in import_data),
                'end': max(record['timestamp'] for record in import_data)
            }
        },
        'daily_statistics': dict(daily_stats),
        'hourly_distribution': dict(hourly_stats),
        'category_distribution': dict(category_totals)
    }

def generate_behavior_analysis(import_data):
    """ç”Ÿæˆè¡Œä¸ºåˆ†ææ•°æ®"""
    
    # è¡Œä¸ºæ¨¡å¼åˆ†æ
    behavior_patterns = {}
    
    # æŒ‰ç±»åˆ«åˆ†æ
    for category in set(record['category'] for record in import_data):
        category_records = [r for r in import_data if r['category'] == category]
        
        # æ—¶é—´åˆ†å¸ƒ
        hours = [datetime.fromisoformat(r['timestamp']).hour for r in category_records]
        hour_distribution = defaultdict(int)
        for hour in hours:
            hour_distribution[hour] += 1
        
        # ç½®ä¿¡åº¦åˆ†æ
        confidences = [r['confidence'] for r in category_records]
        avg_confidence = sum(confidences) / len(confidences)
        
        # æè¿°é•¿åº¦åˆ†æ
        descriptions = [r['reasons'].get('reasons', '') for r in category_records]
        avg_description_length = sum(len(desc) for desc in descriptions) / len(descriptions)
        
        behavior_patterns[category] = {
            'count': len(category_records),
            'percentage': len(category_records) / len(import_data) * 100,
            'avg_confidence': avg_confidence,
            'hour_distribution': dict(hour_distribution),
            'avg_description_length': avg_description_length,
            'peak_hours': sorted(hour_distribution.items(), key=lambda x: x[1], reverse=True)[:3]
        }
    
    # è¡Œä¸ºè½¬æ¢åˆ†æ
    transitions = defaultdict(int)
    for i in range(len(import_data) - 1):
        current_category = import_data[i]['category']
        next_category = import_data[i + 1]['category']
        transitions[f"{current_category} -> {next_category}"] += 1
    
    return {
        'behavior_patterns': behavior_patterns,
        'behavior_transitions': dict(transitions),
        'insights': generate_behavior_insights(behavior_patterns),
        'recommendations': generate_recommendations(behavior_patterns)
    }

def generate_behavior_insights(behavior_patterns):
    """ç”Ÿæˆè¡Œä¸ºæ´å¯Ÿ"""
    
    insights = []
    
    # æœ€æ´»è·ƒçš„è¡Œä¸º
    most_active = max(behavior_patterns.items(), key=lambda x: x[1]['count'])
    insights.append(f"æœ€å¸¸è§çš„è¡Œä¸ºæ˜¯'{most_active[0]}'ï¼Œå æ€»æ´»åŠ¨çš„{most_active[1]['percentage']:.1f}%")
    
    # æœ€é«˜ç½®ä¿¡åº¦çš„è¡Œä¸º
    highest_confidence = max(behavior_patterns.items(), key=lambda x: x[1]['avg_confidence'])
    insights.append(f"ç½®ä¿¡åº¦æœ€é«˜çš„è¡Œä¸ºæ˜¯'{highest_confidence[0]}'ï¼Œå¹³å‡ç½®ä¿¡åº¦ä¸º{highest_confidence[1]['avg_confidence']:.2f}")
    
    # æ´»è·ƒæ—¶æ®µåˆ†æ
    all_hours = defaultdict(int)
    for pattern in behavior_patterns.values():
        for hour, count in pattern['hour_distribution'].items():
            all_hours[hour] += count
    
    peak_hour = max(all_hours.items(), key=lambda x: x[1])
    insights.append(f"æœ€æ´»è·ƒçš„æ—¶æ®µæ˜¯{peak_hour[0]}:00-{peak_hour[0]}:59ï¼Œå…±æœ‰{peak_hour[1]}æ¬¡æ´»åŠ¨")
    
    return insights

def generate_recommendations(behavior_patterns):
    """ç”Ÿæˆå»ºè®®"""
    
    recommendations = []
    
    # åŸºäºè¡Œä¸ºåˆ†å¸ƒçš„å»ºè®®
    explore_count = behavior_patterns.get('explore', {}).get('count', 0)
    observe_count = behavior_patterns.get('observe', {}).get('count', 0)
    
    if explore_count > observe_count:
        recommendations.append("å® ç‰©è¡¨ç°å‡ºè¾ƒå¼ºçš„æ¢ç´¢æ¬²æœ›ï¼Œå»ºè®®æä¾›æ›´å¤šæ–°ç¯å¢ƒå’Œç©å…·")
    else:
        recommendations.append("å® ç‰©æ›´å€¾å‘äºè§‚å¯Ÿï¼Œå»ºè®®åˆ›é€ å®‰å…¨çš„è§‚å¯Ÿç¯å¢ƒ")
    
    # åŸºäºç½®ä¿¡åº¦çš„å»ºè®®
    low_confidence_behaviors = [
        name for name, data in behavior_patterns.items() 
        if data['avg_confidence'] < 0.7
    ]
    
    if low_confidence_behaviors:
        recommendations.append(f"ä»¥ä¸‹è¡Œä¸ºçš„è¯†åˆ«ç½®ä¿¡åº¦è¾ƒä½ï¼Œå»ºè®®æ”¹å–„ç›‘æ§æ¡ä»¶: {', '.join(low_confidence_behaviors)}")
    
    return recommendations

def generate_timeline_data(import_data):
    """ç”Ÿæˆæ—¶é—´çº¿æ•°æ®"""
    
    timeline = []
    
    for record in import_data:
        dt = datetime.fromisoformat(record['timestamp'])
        
        timeline_item = {
            'timestamp': record['timestamp'],
            'time_formatted': dt.strftime('%H:%M:%S'),
            'date_formatted': dt.strftime('%Yå¹´%mæœˆ%dæ—¥'),
            'category': record['category'],
            'category_chinese': get_category_chinese(record['category']),
            'confidence': record['confidence'],
            'confidence_percentage': int(record['confidence'] * 100),
            'description': record['reasons'].get('reasons', ''),
            'metadata': record['reasons']
        }
        
        timeline.append(timeline_item)
    
    return {
        'timeline': timeline,
        'summary': {
            'total_events': len(timeline),
            'duration': str(datetime.fromisoformat(import_data[-1]['timestamp']) - 
                           datetime.fromisoformat(import_data[0]['timestamp'])),
            'categories': list(set(item['category'] for item in timeline))
        }
    }

def get_category_chinese(category):
    """è·å–ç±»åˆ«çš„ä¸­æ–‡åç§°"""
    category_map = {
        'explore': 'æ¢ç´¢',
        'observe': 'è§‚æœ›',
        'neutral': 'ä¸­æ€§',
        'no_pet': 'æ— å® ç‰©',
        'occupy': 'å æ®',
        'attack': 'æ”»å‡»',
        'play': 'ç©è€',
        'sleep': 'ç¡è§‰',
        'eat': 'è¿›é£Ÿ',
        'drink': 'é¥®æ°´',
        'groom': 'æ¢³ç†',
        'rest': 'ä¼‘æ¯'
    }
    return category_map.get(category, category)

if __name__ == "__main__":
    result = create_final_import_files()
    
    print(f"\n=== å¯¼å…¥éªŒè¯æ‘˜è¦ ===")
    print(f"æ€»è®°å½•æ•°: {result['total_records']}")
    print(f"åˆ›å»ºæ–‡ä»¶æ•°: {result['files_created']}")
    print(f"ç»Ÿè®¡ç±»åˆ«æ•°: {len(result['statistics']['category_distribution'])}")
    print(f"è¡Œä¸ºæ¨¡å¼æ•°: {len(result['behavior_analysis']['behavior_patterns'])}")
    
    print(f"\n=== ä¸‹ä¸€æ­¥æ“ä½œ ===")
    print("1. åœ¨åº”ç”¨ç¨‹åºä¸­å¯¼èˆªåˆ°å†å²è®°å½•é¡µé¢")
    print("2. ç‚¹å‡»å¯¼å…¥æŒ‰é’®ï¼Œé€‰æ‹© 'final_pet_activity_data.txt' æ–‡ä»¶")
    print("3. éªŒè¯æ•°æ®åœ¨å†å²è®°å½•ä¸­çš„æ˜¾ç¤º")
    print("4. æ£€æŸ¥ç»Ÿè®¡æŠ¥è¡¨é¡µé¢çš„æ•°æ®å±•ç¤º")
    print("5. éªŒè¯è¡Œä¸ºåˆ†æé¡µé¢çš„æ•°æ®ä¸€è‡´æ€§")